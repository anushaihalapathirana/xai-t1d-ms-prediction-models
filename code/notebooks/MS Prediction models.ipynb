{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca9ab5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy\n",
    "import missingno as msno\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import shap\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import RFE\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import lightgbm as ltb\n",
    "import catboost as ctb\n",
    "from skrebate import ReliefF\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif, SelectPercentile\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, auc, roc_curve, roc_auc_score, precision_score, recall_score, balanced_accuracy_score\n",
    "from numpy.random import seed\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score, KFold, StratifiedKFold, cross_validate\n",
    "seed(42)\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "from tabulate import tabulate\n",
    "\n",
    "numpy.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03527e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run helper functions\n",
    "%run Helper.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d78ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "df = pd.read_csv('../data/data_ms.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5879090",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3276fbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_variable = 'relapse.2y.after.study' # change the response variable\n",
    "correlated_variables = find_correlated_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49b6e51",
   "metadata": {},
   "source": [
    "# Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de9de3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out baseline features - features mentioned in the paper\n",
    "df_baseline = df[['nr.relapses.2y.prior.study', 'edss', 'disease.duration', 'treatment.naive.prior.start.cycle',\n",
    "                  'age', 'nr.Gd.enhanced.lesions', 'months.since.last.relapse', 'gender', 'treatment.during.cycle', \n",
    "                  'patient.id',\n",
    "                  response_variable\n",
    "                ]]\n",
    "\n",
    "# Data preprocessing with baseline features\n",
    "df_baseline, X_train, X_test, Y_train, Y_test, X, Y,  X_train_imputed, X_test_imputed = preprocessing(df_baseline,\n",
    "                                                                                                      0.25, \n",
    "                                                                                                      response_variable,\n",
    "                                                                                                      isms = True,\n",
    "                                                                                                      skip_corr = True)\n",
    "\n",
    "\n",
    "# Train models\n",
    "\n",
    "xgboost =XGBClassifier(\n",
    "        use_label_encoder=False,\n",
    "        eta = 0.02,\n",
    "        max_depth = 1, \n",
    "        max_delta_step = 1,\n",
    "        subsample = 0.5,\n",
    "        colsample_bytree = 1,\n",
    "        tree_method = \"auto\",\n",
    "        process_type = \"default\",\n",
    "        num_parallel_tree=7,\n",
    "        objective='multi:softmax',\n",
    "        min_child_weight = 5,\n",
    "        booster='gbtree',\n",
    "        eval_metric = \"mlogloss\",\n",
    "        num_class = 2\n",
    "    )\n",
    "\n",
    "lgb = ltb.LGBMClassifier(use_missing = True,\n",
    "                         learning_rate = 0.02,\n",
    "                         max_depth =2, random_state=0)\n",
    "\n",
    "catboost = ctb.CatBoostClassifier(iterations=10,\n",
    "                          learning_rate=0.2,\n",
    "                          scale_pos_weight=4,\n",
    "                          loss_function='Logloss',\n",
    "                          eval_metric='AUC',\n",
    "                          depth=2)\n",
    "\n",
    "adaboost = AdaBoostClassifier(random_state=0, learning_rate=0.01, n_estimators=500)\n",
    "\n",
    "rf = RandomForestClassifier(max_depth=2,\n",
    "                             n_estimators = np.shape(X_train)[1],\n",
    "                             criterion = 'entropy', # {“gini”, “entropy”}, default=”gini”\n",
    "                             class_weight = 'balanced_subsample', # {“balanced”, “balanced_subsample”}, dict or list of dicts, default=None\n",
    "                             ccp_alpha=0.008,\n",
    "                             random_state=0)\n",
    "\n",
    "lr = LogisticRegression(penalty='l2',\n",
    "                        tol = 1e-3,\n",
    "                        C=1,\n",
    "                        l1_ratio = 0.1,\n",
    "                        class_weight='balanced',\n",
    "                        random_state=0,\n",
    "                        solver = 'saga')\n",
    "\n",
    "models = [xgboost, lgb, catboost, adaboost, rf, lr]\n",
    "\n",
    "trained_models = init(models, X_train, X_test, X_train_imputed, X_test_imputed, Y_train, Y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f068aa",
   "metadata": {},
   "source": [
    "# Approach 1 - Without feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c26c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_app1 = original_df\n",
    "\n",
    "# data preprocessing\n",
    "df_app1, X_train, X_test, Y_train, Y_test, X, Y,  X_train_imputed, X_test_imputed = preprocessing(df_app1,\n",
    "                                                                                                  0.25,\n",
    "                                                                                                  response_variable,\n",
    "                                                                                                  isms = True)\n",
    "\n",
    "# Tune parameters accordingly\n",
    "xgboost =XGBClassifier(\n",
    "        use_label_encoder=False,\n",
    "        eta = 0.1,\n",
    "        max_depth = 4,\n",
    "        max_delta_step = 10,\n",
    "        subsample = 0.5,\n",
    "        colsample_bytree = 1,\n",
    "        tree_method = \"auto\",\n",
    "        process_type = \"default\",\n",
    "        num_parallel_tree=7,\n",
    "        objective='multi:softmax',\n",
    "        min_child_weight = 3,\n",
    "        booster='gbtree',\n",
    "        eval_metric = \"mlogloss\",\n",
    "        alpha  = 0.08,\n",
    "        num_class = 2\n",
    "    )\n",
    "\n",
    "lgb = ltb.LGBMClassifier(use_missing = True, \n",
    "                         learning_rate = 0.01, \n",
    "                         scale_pos_weight=1,\n",
    "                         max_depth =4, random_state=0 )\n",
    "\n",
    "catboost = ctb.CatBoostClassifier(iterations=10,\n",
    "                          learning_rate=0.1,\n",
    "                          scale_pos_weight=0.4,\n",
    "                          depth=3)\n",
    "\n",
    "adaboost = AdaBoostClassifier(random_state=0,\n",
    "                              learning_rate=0.1,\n",
    "                              n_estimators=1000,\n",
    "                              algorithm = \"SAMME.R\") \n",
    "\n",
    "rf = RandomForestClassifier(max_depth=4,\n",
    "                             n_estimators = np.shape(X_train)[1] ,\n",
    "                             criterion = 'gini',\n",
    "                             class_weight = 'balanced',\n",
    "                             ccp_alpha=0.01,\n",
    "                             random_state=0)\n",
    "\n",
    "lr = LogisticRegression(\n",
    "    penalty='l2',\n",
    "    tol = 5e-4,\n",
    "    C=4,\n",
    "    l1_ratio = 0.1,\n",
    "    class_weight='balanced',\n",
    "    random_state=0,\n",
    "    solver = 'saga'\n",
    ")\n",
    "\n",
    "models = [xgboost, lgb, catboost, adaboost, rf, lr]\n",
    "\n",
    "trained_models_case1 = init(models, X_train, X_test, X_train_imputed, X_test_imputed, Y_train, Y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36ba5df",
   "metadata": {},
   "source": [
    "# Approach 2 - With feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8552cf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the dataset into two based on sex before feature selection. Use this in sex-stratified models\n",
    "df_app2 = original_df\n",
    "\n",
    "# Data preprocessing\n",
    "df_app2, X_train, X_test, Y_train, Y_test, X, Y,  X_train_imputed, X_test_imputed = preprocessing(df_app2,\n",
    "                                                                                                  0.25,\n",
    "                                                                                                  response_variable,\n",
    "                                                                                                  isms = True)\n",
    "\n",
    "# Make copy of training and testing data\n",
    "train_ = X_train.copy()\n",
    "test_ = X_test.copy()\n",
    "train_imputed_ = X_train_imputed.copy()\n",
    "test_imputed_ = X_test_imputed.copy()\n",
    "Y_train_ = Y_train.copy()\n",
    "Y_test_ = Y_test.copy()\n",
    "\n",
    "train_[response_variable] = Y_train_\n",
    "test_[response_variable] = Y_test_\n",
    "train_imputed_[response_variable] = Y_train_\n",
    "test_imputed_[response_variable] = Y_test_\n",
    "\n",
    "# get sex stratified data\n",
    "female_X_train, female_X_train_imputed, female_Y_train, female_X_test, female_X_test_imputed, female_Y_test = get_gender_stratified_data(1, train_, test_, \n",
    "    train_imputed_, test_imputed_, Y_train_, Y_test_, 'gender')\n",
    "male_X_train, male_X_train_imputed, male_Y_train, male_X_test, male_X_test_imputed, male_Y_test = get_gender_stratified_data(0,  train_, test_,\n",
    "    train_imputed_, test_imputed_, Y_train_, Y_test_, 'gender')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a0cf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection using ReliefF method\n",
    "best_n = 10 # number of features \n",
    "\n",
    "features = get_important_features(best_n, X_train.to_numpy(), Y_train.to_numpy())\n",
    "display(features)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21295385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns with selected features\n",
    "X_train_feat = X_train[features]\n",
    "X_test_feat = X_test[features]\n",
    "\n",
    "X_train_feat_imputed_ = X_train_imputed[features]\n",
    "X_test_feat_imputed_ = X_test_imputed[features]\n",
    "\n",
    "# Train models. use same models in approach 1\n",
    "models = [xgboost, lgb, catboost, adaboost, rf, lr]\n",
    "\n",
    "trained_models_case2 = init(models, X_train_feat, X_test_feat, X_train_feat_imputed_, X_test_feat_imputed_, Y_train, Y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aedbbb1",
   "metadata": {},
   "source": [
    "# Approach 3 - Sex-stratified models with selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb021486",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_app2.copy()\n",
    "# print gender count in original preprocessed dataset\n",
    "def get_gender_count(df):\n",
    "    gender_groupby = df.groupby('gender', as_index=False).agg(total= ('patient.id','count'))\n",
    "    print(gender_groupby)\n",
    "\n",
    "get_gender_count(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5219fee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_n = 10 # number of features \n",
    "features = get_important_features(best_n, female_X_train.to_numpy(), female_Y_train.to_numpy())\n",
    "print(features)\n",
    "\n",
    "# feat_names = [\n",
    "#  'nr.relapses.2y.prior.study',\n",
    "#  'kfs.7',\n",
    "#  'kfs.3',\n",
    "#  'edss',\n",
    "#  'disease.duration',\n",
    "#  'age',\n",
    "#  'lesion_count_3mmfilter',\n",
    "#  'age.at.diag',\n",
    "#  'nr.Gd.enhanced.lesions',\n",
    "#  'months.since.last.relapse']\n",
    "\n",
    "\n",
    "# Select columns with selected features in sex-stratified data\n",
    "female_X_train_feat = female_X_train[features]\n",
    "female_X_test_feat = female_X_test[features]\n",
    "\n",
    "female_X_train_imputed_feat = female_X_train_imputed[features]\n",
    "female_X_test_imputed_feat = female_X_test_imputed[features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3be96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train female models\n",
    "\n",
    "xgboost=XGBClassifier(\n",
    "        use_label_encoder=False,\n",
    "        eta = 0.01,#eta between(0.01-0.2)\n",
    "        max_depth =4, #values between(3-10)\n",
    "        max_delta_step =8,\n",
    "        subsample = 0.3,#values between(0.5-1)\n",
    "        colsample_bytree = 0.9,#values between(0.5-1)\n",
    "        tree_method = \"exact\", # {'approx', 'auto', 'exact', 'gpu_hist', 'hist'}\n",
    "        process_type = \"default\",\n",
    "        num_parallel_tree=6,\n",
    "        objective='multi:softmax',\n",
    "        min_child_weight = 15,\n",
    "        booster='gbtree',\n",
    "        alpha  = 0.08,\n",
    "        eval_metric = \"logloss\",\n",
    "        num_class = 2\n",
    "    )\n",
    "\n",
    "lgb = ltb.LGBMClassifier(use_missing = True,\n",
    "                         learning_rate =0.01,\n",
    "                         max_depth =3, \n",
    "                         random_state=0)\n",
    "\n",
    "catboost = ctb.CatBoostClassifier(iterations=11,\n",
    "                          learning_rate=0.085,\n",
    "                          scale_pos_weight=0.82,\n",
    "                          eval_metric='AUC',\n",
    "                          loss_function='Logloss',\n",
    "                          depth=3)\n",
    "\n",
    "adaboost = AdaBoostClassifier(random_state=0,\n",
    "                           algorithm= 'SAMME.R',\n",
    "                           learning_rate=0.01,\n",
    "                           n_estimators=150)\n",
    "\n",
    "rf = RandomForestClassifier(max_depth=5,\n",
    "                             n_estimators = np.shape(female_X_train_feat)[1],\n",
    "                             criterion = 'entropy',\n",
    "                             class_weight = 'balanced_subsample', \n",
    "                             ccp_alpha=0.01,\n",
    "                             random_state=0)\n",
    "\n",
    "lr = LogisticRegression( penalty='l2',\n",
    "                        tol =0.01,\n",
    "                        C=0.1,\n",
    "                        l1_ratio =0.2,\n",
    "                        class_weight='balanced',\n",
    "                        random_state=0,\n",
    "                        solver = 'saga'\n",
    "                    )\n",
    "\n",
    "models = [xgboost, lgb, catboost, adaboost, rf, lr]\n",
    "    \n",
    "trained_models_case3_female = init(models, female_X_train_feat, female_X_test_feat,\n",
    "                                   female_X_train_imputed_feat, female_X_test_imputed_feat, \n",
    "                                   female_Y_train, female_Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503f66dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_n = 10 # number of features \n",
    "\n",
    "features = get_important_features(best_n, male_X_train.to_numpy(), male_Y_train.to_numpy())\n",
    "print(features)\n",
    "\n",
    "## Below are the selected features\n",
    "# feat_names = ['nr.relapses.2y.prior.study',\n",
    "#  'kfs.3',\n",
    "#  'edss',\n",
    "#  'disease.duration',\n",
    "#  'treatment.naive.prior.start.cycle',\n",
    "#  'age',\n",
    "#  'age.at.diag',\n",
    "#  'nr.Gd.enhanced.lesions',\n",
    "#  'ave.nine.hole.peg.test',\n",
    "#  'months.since.last.relapse']\n",
    "\n",
    "male_X_train_feat = male_X_train[features]\n",
    "male_X_test_feat = male_X_test[features]\n",
    "\n",
    "male_X_train_imputed_feat = male_X_train_imputed[features]\n",
    "male_X_test_imputed_feat = male_X_test_imputed[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73221f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train male models \n",
    "\n",
    "xgboost=XGBClassifier(\n",
    "        use_label_encoder=False,\n",
    "        eta = 0.02,#eta between(0.01-0.2)\n",
    "        max_depth =4, #values between(3-10)\n",
    "        max_delta_step =20,\n",
    "        subsample = 0.3,#values between(0.5-1)\n",
    "        colsample_bytree = 0.9,#values between(0.5-1)\n",
    "        tree_method = \"approx\", # {'approx', 'auto', 'exact', 'gpu_hist', 'hist'}\n",
    "        process_type = \"default\",\n",
    "        num_parallel_tree=7,\n",
    "        objective='multi:softmax',\n",
    "        min_child_weight = 11,\n",
    "        booster='gbtree',\n",
    "        alpha  = 0.01,\n",
    "        eval_metric = \"logloss\",\n",
    "        num_class = 2\n",
    "    )\n",
    "\n",
    "lgb = ltb.LGBMClassifier(use_missing = True,\n",
    "                         learning_rate =0.01,\n",
    "                         max_depth = 1, \n",
    "                         random_state=0)\n",
    "\n",
    "catboost = ctb.CatBoostClassifier(iterations=16,\n",
    "                                  learning_rate=0.01,\n",
    "                                  scale_pos_weight=0.95,\n",
    "                                  eval_metric='Accuracy',\n",
    "                                  loss_function='Logloss',\n",
    "                                  depth=3)\n",
    "\n",
    "adaboost = AdaBoostClassifier(random_state=0,\n",
    "                           algorithm= 'SAMME.R',\n",
    "                           learning_rate=0.02,\n",
    "                           n_estimators=175)\n",
    "\n",
    "rf = RandomForestClassifier(max_depth=4,\n",
    "                             n_estimators = np.shape(male_X_train_feat)[1],\n",
    "                             criterion = 'entropy',\n",
    "                             class_weight = 'balanced_subsample', \n",
    "                             ccp_alpha=0.02,\n",
    "                             random_state=0)\n",
    "\n",
    "lr = LogisticRegression(penalty='l2',\n",
    "                        tol =0.1,\n",
    "                        C=0.09,\n",
    "                        l1_ratio = 0.5,\n",
    "                        class_weight='balanced', \n",
    "                        random_state=0,\n",
    "                        solver = 'saga' \n",
    "                    )\n",
    "\n",
    "models = [xgboost, lgb, catboost, adaboost, rf, lr]\n",
    "\n",
    "trained_models_case3_male = init(models, male_X_train_feat, male_X_test_feat,\n",
    "                                   male_X_train_imputed_feat, male_X_test_imputed_feat, \n",
    "                                   male_Y_train, male_Y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5205b9a8",
   "metadata": {},
   "source": [
    "# SHAP plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b28cdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global SHAP plots - FEMALE\n",
    "\n",
    "# Select the model you wanted to explain\n",
    "# select XGBClassifier in Approach 2\n",
    "selected_model = trained_models_case3_female['CatBoostClassifier']\n",
    "\n",
    "# Calculate shap values\n",
    "explainer = shap.Explainer(selected_model)\n",
    "shap_values = explainer(female_X_train_feat)\n",
    "# shap_values = shap_values[:, :, 0]\n",
    "shap.summary_plot(shap_values, max_display=20, show=False, plot_size=[15,15])\n",
    "\n",
    "fig, ax = plt.gcf(), plt.gca()\n",
    "ax.tick_params(labelsize=18)\n",
    "ax.set_xlabel(\"SHAP value (impact on model output)\", fontsize=18)\n",
    "cb_ax = fig.axes[1] \n",
    "cb_ax.tick_params(labelsize=18)\n",
    "cb_ax.set_ylabel(\"Feature value\", fontsize=18)\n",
    "plt.savefig(\"figures/MS_FEMALE_Global_SHAP_plot.jpeg\" ,bbox_inches='tight', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7605e796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local SHAP plot - FEMALE\n",
    "\n",
    "# select the data sample in test data\n",
    "row_to_show = 84\n",
    "data_for_prediction = female_X_test_feat.iloc[row_to_show]  # use 1 row of data here. Could use multiple rows if desired\n",
    "data_for_prediction_array = data_for_prediction.values.reshape(1, -1)\n",
    "\n",
    "explainer = shap.TreeExplainer(selected_model, check_additivity=False,  feature_perturbation='interventional' )\n",
    "shap_values = explainer.shap_values(female_X_test_feat.iloc[[row_to_show]])\n",
    "\n",
    "\n",
    "# Modified the force plot to improve clarity and highlight feature interactions\n",
    "# Extract the SHAP values for class 0\n",
    "shap_values_class0 = shap_values[0,]\n",
    "\n",
    "# Map feature names to corresponding data values\n",
    "mapping = dict(zip(X_test_feat.columns.tolist(), list(data_for_prediction)))\n",
    "feature_data = [f\"{feature}={mapping[feature]}\" for feature in mapping]\n",
    "\n",
    "# Filter out zero SHAP values and corresponding feature data\n",
    "sv_ = []\n",
    "features_ = []\n",
    "for sv, feature in zip(shap_values_class0, feature_data):\n",
    "    sv_.append(sv)\n",
    "    features_.append(feature)\n",
    "\n",
    "# Sort the features and SHAP values in descending order\n",
    "sorted_features = [feature for _, feature in sorted(zip(sv_, features_), reverse=True)]\n",
    "sorted_sv = sorted(sv_, reverse=True)\n",
    "\n",
    "# Set the threshold to filter out small SHAP values\n",
    "threshold = 0.00\n",
    "\n",
    "# Assign colors based on positive or negative SHAP values\n",
    "colors = ['crimson' if sv >= 0 else 'dodgerblue' for sv in sorted_sv]\n",
    "\n",
    "# Create the bar plot\n",
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "ax.tick_params(labelsize=18)\n",
    "ax.barh(sorted_features, sorted_sv, color=colors, left=threshold)\n",
    "\n",
    "# Show top values at the top of the plot\n",
    "ax.invert_yaxis()\n",
    "plt.xlabel('SHAP value', fontsize=18)\n",
    "plt.ylabel('Feature value', fontsize=18)\n",
    "plt.savefig(\"figures/MS_FEMALE_Local_SHAP_plot.jpeg\", bbox_inches='tight', dpi=300)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3305b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global SHAP plots - MALE\n",
    "\n",
    "# Select the model you wanted to explain\n",
    "# select XGBClassifier in Approach 2\n",
    "selected_model = trained_models_case3_male['XGBClassifier']\n",
    "\n",
    "# Calculate shap values\n",
    "explainer = shap.Explainer(selected_model, male_X_train_feat)\n",
    "shap_values = explainer(male_X_train_feat)\n",
    "shap_values = shap_values[:, :, 1]\n",
    "shap.summary_plot(shap_values, max_display=20, show=False, plot_size=[15,15])\n",
    "\n",
    "fig, ax = plt.gcf(), plt.gca()\n",
    "ax.tick_params(labelsize=18)\n",
    "ax.set_xlabel(\"SHAP value (impact on model output)\", fontsize=18)\n",
    "cb_ax = fig.axes[1] \n",
    "cb_ax.tick_params(labelsize=18)\n",
    "cb_ax.set_ylabel(\"Feature value\", fontsize=18)\n",
    "plt.savefig(\"figures/MS_MALE_Global_SHAP_plot.jpeg\" ,bbox_inches='tight', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36531fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local SHAP plot - MALE\n",
    "\n",
    "# select the data sample in test data\n",
    "row_to_show = 60\n",
    "data_for_prediction = male_X_test_feat.iloc[row_to_show]  # use 1 row of data here. Could use multiple rows if desired\n",
    "data_for_prediction_array = data_for_prediction.values.reshape(1, -1)\n",
    "\n",
    "explainer = shap.TreeExplainer(selected_model, male_X_test_feat, check_additivity=False,  feature_perturbation='interventional' )\n",
    "shap_values = explainer.shap_values(male_X_test_feat.iloc[[row_to_show]])\n",
    "\n",
    "# Modified the force plot to improve clarity and highlight feature interactions\n",
    "# Extract the SHAP values for class 0\n",
    "shap_values_class0 = shap_values[1][0,]\n",
    "\n",
    "# Map feature names to corresponding data values\n",
    "mapping = dict(zip(X_test_feat.columns.tolist(), list(data_for_prediction)))\n",
    "feature_data = [f\"{feature}={mapping[feature]}\" for feature in mapping]\n",
    "\n",
    "# Filter out zero SHAP values and corresponding feature data\n",
    "sv_ = []\n",
    "features_ = []\n",
    "for sv, feature in zip(shap_values_class0, feature_data):\n",
    "    sv_.append(sv)\n",
    "    features_.append(feature)\n",
    "\n",
    "# Sort the features and SHAP values in descending order\n",
    "sorted_features = [feature for _, feature in sorted(zip(sv_, features_), reverse=True)]\n",
    "sorted_sv = sorted(sv_, reverse=True)\n",
    "\n",
    "# Set the threshold to filter out small SHAP values\n",
    "threshold = 0.00\n",
    "\n",
    "# Assign colors based on positive or negative SHAP values\n",
    "colors = ['crimson' if sv >= 0 else 'dodgerblue' for sv in sorted_sv]\n",
    "\n",
    "# Create the bar plot\n",
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "ax.tick_params(labelsize=18)\n",
    "ax.barh(sorted_features, sorted_sv, color=colors, left=threshold)\n",
    "\n",
    "# Show top values at the top of the plot\n",
    "ax.invert_yaxis()\n",
    "plt.xlabel('SHAP value', fontsize=18)\n",
    "plt.ylabel('Feature value', fontsize=18)\n",
    "plt.savefig(\"figures/MS_MALE_Local_SHAP_plot.jpeg\", bbox_inches='tight', dpi=300)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
